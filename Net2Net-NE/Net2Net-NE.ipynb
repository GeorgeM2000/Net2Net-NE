{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/Net2Net-NE/blob/master/Net2Net-NE/Net2Net-NE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18f22bd",
      "metadata": {
        "id": "f18f22bd"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a170c362",
      "metadata": {
        "id": "a170c362"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import gc\n",
        "#import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "else:\n",
        "    print(\"CUDA is not available, using CPU.\")"
      ],
      "metadata": {
        "id": "iUXCGPavyb9F"
      },
      "id": "iUXCGPavyb9F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2235cc",
      "metadata": {
        "id": "9a2235cc"
      },
      "outputs": [],
      "source": [
        "gpu_id = 0\n",
        "gpu = torch.device('cuda', gpu_id)\n",
        "print(gpu)  # Should print: cuda:0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.get_device_name(gpu))"
      ],
      "metadata": {
        "id": "OfT0ihrsymcd"
      },
      "id": "OfT0ihrsymcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "RTTLKqhwLi5K"
      },
      "id": "RTTLKqhwLi5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ab58f741",
      "metadata": {
        "id": "ab58f741"
      },
      "source": [
        "# ***Global Variables & General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"arxiv\"\n",
        "parent_path = f'Datasets/{dataset_name}/graph-v2'\n",
        "parent_path = '/content'\n",
        "graph_file = 'edges.txt'\n",
        "categories_file = 'labels.txt'\n",
        "vocab_file = 'voc.txt'\n",
        "\n",
        "data_text_file  = \"abstract.txt\"\n",
        "data_text_files = [\"data-v3-500.txt\", \"data-v3-500C.txt\", \"YAKE10.txt\", \"YAKE5.txt\", \"RAKE10.txt\", \"RAKE5.txt\", \"RAKE10C.txt\", \"RAKE5C.txt\", \"TFIDF10.txt\", \"TFIDF5.txt\", \"PosR5.txt\",\n",
        "                   \"PosR10.txt\", \"TextR5.txt\", \"TextR10.txt\", \"TopicR5.txt\", \"TopicR10.txt\"]\n",
        "\n",
        "log_file               = 'Net2Net-NE_Execution_Logs.txt'\n",
        "link_pred_results_file = 'Net2Net-NE_Link_Pred_Res.txt'\n",
        "node_clf_results_file  = 'Net2Net-NE_Node_Clf_Res.txt'\n",
        "\n",
        "\n",
        "split_graph_file  = 'sgraph15.txt'\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "\n",
        "test_graph_file   = 'tgraph85.txt'\n",
        "test_graph_files  = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']"
      ],
      "metadata": {
        "id": "lNQfzU4eNGmq"
      },
      "id": "lNQfzU4eNGmq",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1339d832",
      "metadata": {
        "id": "1339d832"
      },
      "outputs": [],
      "source": [
        "word_num = 12619\n",
        "MAX_LEN = 100 # Default value for single execution\n",
        "MAX_LENS = [] # List to hold values for multiple executions\n",
        "\n",
        "word_emb_dim = 200 # Original: 500\n",
        "conv_dim = 200 # Orig: 500\n",
        "kernel_num = 200\n",
        "kernel_sizes = [1, 2, 3, 4, 5]\n",
        "conv_drop = 0.2\n",
        "enc_dim = 200 # Orig: 500\n",
        "batch_size = 64\n",
        "epoch_num = 50\n",
        "l_rate = 1e-3\n",
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num = 5\n",
        "train_classifier = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ffdd2d",
      "metadata": {
        "id": "92ffdd2d"
      },
      "outputs": [],
      "source": [
        "# Find the average number of words from each data text file\n",
        "for txtf in ['abstract.txt']: # 1) ['data-v3-500.txt'] 2) data_text_files:\n",
        "    total_word_count = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
        "            total_lines += 1\n",
        "\n",
        "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
        "    MAX_LENS.append(int(math.ceil(mean_word_count)))\n",
        "    print(f'=== {txtf} ===')\n",
        "    print(\"Mean word count:\", math.ceil(mean_word_count))\n",
        "    print(f'Total words: {total_word_count}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59de32f2",
      "metadata": {
        "id": "59de32f2"
      },
      "outputs": [],
      "source": [
        "MAX_LENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7b991536",
      "metadata": {
        "id": "7b991536"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99f28bc",
      "metadata": {
        "id": "e99f28bc"
      },
      "outputs": [],
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\", encoding='utf-8') as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a662bf46",
      "metadata": {
        "id": "a662bf46"
      },
      "outputs": [],
      "source": [
        "zero_list = []\n",
        "for i in range(0, word_emb_dim):\n",
        "    zero_list.append(0)\n",
        "zero_list = np.array(zero_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "with open(f'{parent_path}/{graph_file}', 'r') as f:\n",
        "  eedges = f.readlines()\n",
        "\n",
        "edge_list = []\n",
        "nodes = [] # \"nodes\" will contain all the unique nodes of the graph\n",
        "\n",
        "for ee in eedges:\n",
        "  edge_list.append(list(ee.split()))\n",
        "\n",
        "for ll in edge_list:\n",
        "  for ed in ll:\n",
        "    if ed not in nodes:\n",
        "      nodes.append(ed)\n",
        "    else:\n",
        "      continue"
      ],
      "metadata": {
        "id": "DrhNwj8871Pj"
      },
      "id": "DrhNwj8871Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c36c634c",
      "metadata": {
        "id": "c36c634c"
      },
      "source": [
        "# ***Classify***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f81f8602",
      "metadata": {
        "id": "f81f8602"
      },
      "outputs": [],
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        X_train = [self.embeddings[x] for x in X]\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y]\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        # averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
        "        # f1_results = {}\n",
        "        # pre_results = {}\n",
        "        # rec_results = {}\n",
        "        # acc_results = accuracy_score(Y, Y_)\n",
        "        # f1_macro = f1_score(Y, Y_, average=\"macro\")\n",
        "        f1_micro = f1_score(Y, Y_, average=\"micro\")\n",
        "        # for average in averages:\n",
        "        #      f1_results[average] = f1_score(Y, Y_, average=average)\n",
        "        #     pre_results[average] = precision_score(Y, Y_, average=average)\n",
        "        #     rec_results[average] = recall_score(Y, Y_, average=average)\n",
        "        # print 'Results, using embeddings of dimensionality', len(self.embeddings[X[0]])\n",
        "        # print '-------------------'\n",
        "        # print('\\nF1 Score: ')\n",
        "        # print(f1_results)\n",
        "        # print('\\nPrecision Score:')\n",
        "        # print(pre_results)\n",
        "        # print('\\nRecall Score:')\n",
        "        # print(rec_results)\n",
        "        # print('Accuracy Score:', acc_results)\n",
        "\n",
        "        # return f1_results, pre_results, rec_results, acc_results\n",
        "        return f1_micro\n",
        "        # print '-------------------'\n",
        "\n",
        "    ''' def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results '''\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[x] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "\n",
        "        training_size = int(train_precent * len(X))\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y)\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)\n",
        "\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    node_num, size = [int(x) for x in fin.readline().strip().split()]\n",
        "    vectors = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        vec = l.strip().split(' ')\n",
        "        assert len(vec) == size + 1\n",
        "        vectors[vec[0]] = [float(x) for x in vec[1:]]\n",
        "    fin.close()\n",
        "    assert len(vectors) == node_num\n",
        "    return vectors\n",
        "\n",
        "\n",
        "def read_node_label(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    X = []\n",
        "    Y = []\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        vec = l.strip().split(' ')\n",
        "\n",
        "        if len(vec) == 2:\n",
        "            X.append(int(vec[0])) # X will contain the node IDs\n",
        "            Y.append([int(v) for v in vec[1:]]) # For each node in the file Y will contain its corresponding labels (one or multiple labels)\n",
        "    fin.close()\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd35348",
      "metadata": {
        "id": "7bd35348"
      },
      "source": [
        "# ***Utilities***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2dd874fd",
      "metadata": {
        "id": "2dd874fd"
      },
      "outputs": [],
      "source": [
        "# Read node features from file\n",
        "def read_node_fea(feature_path):\n",
        "    fea = []\n",
        "    fin = open(feature_path, 'r')\n",
        "    for l in fin.readlines():\n",
        "        vec = l.split()\n",
        "        fea.append(np.array([float(x) for x in vec[1:]]))\n",
        "    fin.close()\n",
        "    return np.array(fea, dtype='float32')\n",
        "\n",
        "\n",
        "def read_word_code(text_path, voca_path):\n",
        "\n",
        "    # Read the vocabulary and store each word in that vocabulary in a list\n",
        "    words = []\n",
        "    fin = open(voca_path, 'r')\n",
        "    for l in fin.readlines():\n",
        "        words.append(l.strip())\n",
        "    fin.close()\n",
        "\n",
        "    print(f'Vocabulary size: {len(words)}')\n",
        "    word_map = {words[i]: i for i in range(len(words))} # Each word will have a corresponding index\n",
        "    pad_code = word_map['<eos>'] # The pad_code will be the index of the '<eos>' character\n",
        "\n",
        "    # This segment reads a node's content and creates a token ID list for each node\n",
        "    content_code = []\n",
        "    fin = open(text_path, 'r')\n",
        "    for l in fin.readlines():\n",
        "        info = l.strip().split(' ') # Split the contents\n",
        "        doc_code = [word_map[w] for w in info] # Get the ID (index) of each token/character\n",
        "\n",
        "        # If there is variable length uncomment below\n",
        "        \"\"\" if len(doc_code) > MAX_LEN:\n",
        "            doc_code = doc_code[0: MAX_LEN]\n",
        "        else:\n",
        "            doc_code.extend([pad_code for _ in range(MAX_LEN - len(doc_code))]) \"\"\"\n",
        "\n",
        "        content_code.append(doc_code)\n",
        "\n",
        "    print(f'Node contents: \\n{len(content_code)}\\n{len(content_code[0])}')\n",
        "    return content_code, pad_code, len(words)\n",
        "    # return np.array(content_code, dtype='int')\n",
        "\n",
        "\n",
        "# load_text() method using Tokenizer(). This is my addition to the original code to replace the read_word_code() function\n",
        "''' def load_text(self, text_path):\n",
        "    text_file = open(text_path, 'rb').readlines()\n",
        "    #text_data = [line.strip() for line in text_file]\n",
        "    text_data = [line.decode('utf-8').strip() for line in text_file]\n",
        "\n",
        "    tokenizer = Tokenizer(oov_token=None) # Default: Tokenizer()\n",
        "    tokenizer.fit_on_texts(text_data)\n",
        "\n",
        "    text = tokenizer.texts_to_sequences(text_data)\n",
        "    text = pad_sequences(text, maxlen=MAX_LEN, padding=\"post\", truncating='post') # Default: pad_sequences(text, maxlen=MAX_LEN, padding=\"post\")\n",
        "\n",
        "    num_vocab = len(tokenizer.word_index) + 1  # +1 for padding token\n",
        "    num_nodes = len(text)\n",
        "    return text, num_vocab, num_nodes '''\n",
        "\n",
        "def load_text(self, text_path):\n",
        "    \"\"\"\n",
        "    Adapting with adapt(text_data):\n",
        "\n",
        "    vectorize_layer.adapt(text_data) analyzes text_data, builds a vocabulary, and assigns a unique integer ID to each word based on its frequency (most frequent words get lower IDs).\n",
        "    Transforming with vectorize_layer(text_data):\n",
        "\n",
        "    This maps each word in text_data to its corresponding integer token ID, producing a 2D array where each row represents a sequence of token IDs for a given input line, padded or truncated to MAX_LEN.\n",
        "    \"\"\"\n",
        "    text_file = open(text_path, 'rb').readlines() # Read the file line by line\n",
        "    for a in range(0, len(text_file)):\n",
        "      text_file[a] = str(text_file[a]) # Convert its contents to string format\n",
        "\n",
        "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=None,  # Set a limit if needed\n",
        "        output_mode='int',# The token IDs will be integers\n",
        "        output_sequence_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "    text_data = [line.strip() for line in text_file]\n",
        "    text_data_size = len(text_data)\n",
        "\n",
        "    if \"data-v3-500\" in self.text_filename: # If a file is too large to be processed at once, break it down into batches and apply the .adapt() method to each batch\n",
        "      batch_size = int(text_data_size / 10)\n",
        "      ranges = []\n",
        "      start = 0\n",
        "      while start < text_data_size:\n",
        "          end = min(start + batch_size, text_data_size)  # Ensure the last range includes all remaining abstracts\n",
        "          ranges.append([start, end])\n",
        "          start = end\n",
        "\n",
        "      ranges[-2][1] = ranges[-1][1]\n",
        "      del ranges[-1]\n",
        "\n",
        "      for range in ranges:\n",
        "        vectorize_layer.adapt(text_data[range[0]:range[1]])\n",
        "\n",
        "    else:\n",
        "      vectorize_layer.adapt(text_data)\n",
        "\n",
        "    text = vectorize_layer(text_data).numpy()\n",
        "    num_vocab = len(vectorize_layer.get_vocabulary())\n",
        "    #print(f'Vocabulary: {num_vocab}')\n",
        "    num_nodes = len(text)\n",
        "\n",
        "    return text, num_vocab, num_nodes\n",
        "\n",
        "\n",
        "def fetch(content_code, ids, max_len, pad_code):\n",
        "    # ids are the nodes\n",
        "    # content_code is a list of lists representing the token IDs of a node's content\n",
        "\n",
        "    code = []\n",
        "    for id in ids: # For each node in nodes (ids)\n",
        "        doc_code = content_code[id] # Take the list of token IDs of the current node\n",
        "\n",
        "        # Apply padding or truncate the current node's token IDs to meet the limitation of MAX_LEN\n",
        "        if len(doc_code) > max_len:\n",
        "            doc_code = doc_code[0: max_len]\n",
        "        else:\n",
        "            doc_code.extend([pad_code for _ in range(max_len - len(doc_code))])\n",
        "\n",
        "        code.append(doc_code)\n",
        "\n",
        "    return code\n",
        "\n",
        "\n",
        "def node_classification(hidden, idx, label, ratio):\n",
        "    lr = Classifier(vectors=hidden, clf=LogisticRegression())\n",
        "    res = lr.split_train_evaluate(idx, label, ratio)\n",
        "    return res\n",
        "\n",
        "# It combines multiple lists (that are inside another list, a 2D list) into a single list of unique elements, i.e., the union of all elements.\n",
        "def exclusive_combine(*in_list):\n",
        "    res = set()\n",
        "    in_list = list(*in_list)\n",
        "    for n_l in in_list:\n",
        "        for i in n_l:\n",
        "            res.add(i)\n",
        "    return list(res)\n",
        "\n",
        "\n",
        "def identity_map(n_list):\n",
        "    id_dict = {}\n",
        "    for i in range(len(n_list)):\n",
        "        id_dict[n_list[i]] = i\n",
        "    return id_dict\n",
        "\n",
        "\n",
        "def agg_mean(M, id_dict, keys):\n",
        "    idList = []\n",
        "    for id in keys:\n",
        "        idList.append(id_dict[id])\n",
        "\n",
        "    return torch.mean(M[idList, :], 0, True)\n",
        "\n",
        "\n",
        "def agg_max(M, id_dict, keys):\n",
        "    idList = []\n",
        "    for id in keys:\n",
        "        idList.append(id_dict[id])\n",
        "    res, _ = torch.max(M[idList, :], 0, True)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a58219",
      "metadata": {
        "id": "b3a58219"
      },
      "source": [
        "# ***Graph***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e67b40dd",
      "metadata": {
        "id": "e67b40dd"
      },
      "outputs": [],
      "source": [
        "class MyGraph(object):\n",
        "    def __init__(self, path, edgelist=True):\n",
        "        self.neighbor_dict = {}\n",
        "\n",
        "        # Create the graph\n",
        "        if edgelist:\n",
        "            fin = open(path, 'r')\n",
        "            for l in fin.readlines():\n",
        "                e = l.split()\n",
        "                i, j = int(e[0]), int(e[1])\n",
        "\n",
        "                # Undirected edges\n",
        "                self.update_edge(i, j)\n",
        "                self.update_edge(j, i)\n",
        "            fin.close()\n",
        "\n",
        "        # Convert each node's neighbors from dict to list\n",
        "        for key in self.neighbor_dict.keys():\n",
        "            self.neighbor_dict[key] = list(self.neighbor_dict[key])\n",
        "\n",
        "        self.node_list = list(self.neighbor_dict.keys()) # node_list will contain all the node IDs sorted\n",
        "        self.node_list.sort()\n",
        "        self.node_num = len(self.node_list)\n",
        "\n",
        "    def update_edge(self, i, j):\n",
        "        if i in self.neighbor_dict:\n",
        "            self.neighbor_dict[i].add(j)\n",
        "        else:\n",
        "            self.neighbor_dict[i] = {j}\n",
        "\n",
        "        if j in self.neighbor_dict:\n",
        "            self.neighbor_dict[j].add(i)\n",
        "        else:\n",
        "            self.neighbor_dict[j] = {i}\n",
        "\n",
        "    def get_batches(self, batch_size):\n",
        "        # np.random.seed(1)\n",
        "        np.random.shuffle(self.node_list) # Shuffle the node_list\n",
        "        num_batches = self.node_num // batch_size\n",
        "        batch_list = []\n",
        "\n",
        "        # Create \"num_batches\" number of batches\n",
        "        for n in range(num_batches):\n",
        "            batch_list.append(self.node_list[n * batch_size: (n + 1) * batch_size])\n",
        "\n",
        "        # Create a final batch that contains the remaining nodes\n",
        "        if self.node_num > num_batches * batch_size:\n",
        "            batch_list.append(self.node_list[num_batches * batch_size:])\n",
        "\n",
        "        self.node_list.sort() # Sort the node_list again after shuffling it\n",
        "        return batch_list\n",
        "\n",
        "    def get_neighbors(self, in_list): # in_list is a list of node IDs\n",
        "        neighbors = [self.neighbor_dict[i] for i in in_list] # For each node get its neighbors. The result is a 2D list\n",
        "        return exclusive_combine(neighbors)\n",
        "\n",
        "    def diffuse(self, step, nodes):\n",
        "        cur_list = nodes\n",
        "        scale_list = [cur_list]\n",
        "        for s in range(step):\n",
        "            neighbors = self.get_neighbors(cur_list)\n",
        "            cur_list = exclusive_combine([cur_list, neighbors])\n",
        "            scale_list.append(cur_list)\n",
        "        return scale_list  # From now to the past\n",
        "\n",
        "    def statistic(self):\n",
        "        neigh_num = []\n",
        "        for n in self.node_list:\n",
        "            neigh_num.append(len(self.neighbor_dict[n]))\n",
        "\n",
        "        return np.max(neigh_num), np.min(neigh_num), np.mean(neigh_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399b3269",
      "metadata": {
        "id": "399b3269"
      },
      "source": [
        "# ***Models***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "31edca4d",
      "metadata": {
        "id": "31edca4d"
      },
      "outputs": [],
      "source": [
        "class MeanAggregator(nn.Module):\n",
        "\n",
        "    def __init__(self, features, cur_device, gcn=False):\n",
        "\n",
        "        super(MeanAggregator, self).__init__()\n",
        "        self.features = features\n",
        "        self.device = cur_device\n",
        "        self.gcn = gcn\n",
        "\n",
        "    def forward(self, nodes, to_neighs):\n",
        "        samp_neighs = [samp_neigh + [nodes[i]] for i, samp_neigh in enumerate(to_neighs)]\n",
        "\n",
        "        unique_nodes_list = exclusive_combine(samp_neighs)\n",
        "        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n",
        "        # The mask for aggregation\n",
        "        mask = torch.zeros(len(samp_neighs), len(unique_nodes), requires_grad=False, device=self.device)\n",
        "        # The connections\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
        "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
        "        mask[row_indices, column_indices] = 1\n",
        "        # Normalize\n",
        "        num_neigh = mask.sum(1, keepdim=True)\n",
        "        mask = mask.div(num_neigh)\n",
        "\n",
        "        embed_matrix = self.features(unique_nodes_list)\n",
        "        to_feats = mask.mm(embed_matrix)\n",
        "        return to_feats  # node_num * fea_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7fd28ce8",
      "metadata": {
        "id": "7fd28ce8"
      },
      "outputs": [],
      "source": [
        "class EgoEncoder(nn.Module):\n",
        "    def __init__(self, features, feature_dim, embed_dim, graph, aggregator, base_model=None):\n",
        "        super(EgoEncoder, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.feat_dim = feature_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.graph = graph\n",
        "        self.aggregator = aggregator\n",
        "        if base_model is not None:\n",
        "            self.base_model = base_model\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(self.feat_dim, embed_dim))\n",
        "        init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        to_neighs = [self.graph.neighbor_dict[node] for node in nodes]\n",
        "        neigh_feats = self.aggregator.forward(nodes, to_neighs)\n",
        "        combined = neigh_feats\n",
        "        combined.mm(self.weight)\n",
        "        combined = torch.tanh(combined)\n",
        "        return combined  # node_num * emb_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ee5a91ef",
      "metadata": {
        "id": "ee5a91ef"
      },
      "outputs": [],
      "source": [
        "class ContentCNN(nn.Module):\n",
        "    def __init__(self, word_num, word_emb_dim, conv_dim, kernel_num, kernel_sizes, dropout, cur_device):\n",
        "        super(ContentCNN, self).__init__()\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(word_num, word_emb_dim)\n",
        "        self.word_embeddings.weight = nn.Parameter(torch.FloatTensor(word_num, word_emb_dim))\n",
        "        self.word_embeddings.cuda(cur_device)\n",
        "\n",
        "        # CNN with different kernel sizes\n",
        "        self.conv_list = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, word_emb_dim)) for K in kernel_sizes])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # self.fc = nn.Linear(len(kernel_sizes) * kernel_num, conv_dim)\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(len(kernel_sizes) * kernel_num, conv_dim))\n",
        "        self.device = cur_device\n",
        "\n",
        "        init.xavier_uniform_(self.word_embeddings.weight)\n",
        "        init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x_conv = conv(x)\n",
        "        x_act = F.relu(x_conv).squeeze(3)  # (N, Co, W)\n",
        "        x_pool = F.max_pool1d(x_act, x_act.size(2)).squeeze(2)\n",
        "        return x_pool\n",
        "\n",
        "    def forward(self, node_batch):\n",
        "        query = torch.LongTensor(node_batch).cuda(self.device)\n",
        "        x = self.word_embeddings(query)  # (N, W, D)\n",
        "\n",
        "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv_list]  # [(N, Co, W), ...]*len(Ks)\n",
        "\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "        # logit = self.fc(x)  # (N, C)\n",
        "        logit = x.mm(self.weight)\n",
        "        logit = torch.tanh(logit)\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfcacada",
      "metadata": {
        "id": "dfcacada"
      },
      "source": [
        "# ***Net2Net-NE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "95a6c3c5",
      "metadata": {
        "id": "95a6c3c5"
      },
      "outputs": [],
      "source": [
        "class Net2Net(nn.Module):\n",
        "    def __init__(self, global_graph, features, encoder):\n",
        "        super(Net2Net, self).__init__()\n",
        "        self.graph = global_graph\n",
        "        self.node_num = self.graph.node_num\n",
        "        self.embed_dim = encoder.embed_dim\n",
        "        self.features = features\n",
        "        self.encoder = encoder\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(self.embed_dim, self.node_num))\n",
        "        init.xavier_uniform_(self.weight) # Original: init.xavier_uniform(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        embeds = self.encoder(nodes)\n",
        "        scores = embeds.mm(self.weight)\n",
        "        return scores\n",
        "\n",
        "    def loss(self, nodes, labels):\n",
        "        scores = self.forward(nodes)\n",
        "        return self.xent(scores, labels.squeeze())\n",
        "\n",
        "    def evaluate(self, b_list, lab, ratio):\n",
        "        self.eval() # It sets the model into evaluation mode, which: Disables dropout layers and makes batch normalization layers use their learned statistics instead of updating them\n",
        "\n",
        "        hidden = [] # hidden represents the node embeddings\n",
        "        idx = []\n",
        "        for bat in b_list: # For each batch in the batch list\n",
        "            h = self.encoder(bat)\n",
        "            hidden.extend(h.detach().cpu().numpy())\n",
        "            idx.extend(bat) # Adds the node indices from the batch into the idx list. This way, you keep track of which node embedding belongs to which node index\n",
        "\n",
        "        print(f'Node embeddings info: \\nLen:{len(hidden)}\\n{len(hidden[0])}')\n",
        "        print(f'Node indices info: \\nLen:{len(idx)}\\nMin:{min(idx)}\\nMax:{max(idx)}')\n",
        "\n",
        "        res = []\n",
        "        for r in ratio: # For each splitting ratio, e.g., 0.15, 0.45, 0.75\n",
        "            res.append(node_classification(hidden, # Take all the node embeddings\n",
        "                                          np.arange(len(lab)),  # Create a list of all the node IDs. For link prediction, instead of len(lab), use the total number of nodes\n",
        "                                          [lab[i] for i in idx], # For each node in idx take its corresponding label\n",
        "                                          r))\n",
        "        return res\n",
        "\n",
        "    def evaluate_and_save_embeddings(self, b_list, graph, text_filename):\n",
        "      self.eval()\n",
        "\n",
        "      hidden = []\n",
        "      idx = []\n",
        "      for bat in b_list:\n",
        "          h = self.encoder(bat)\n",
        "          hidden.extend(h.detach().cpu().numpy())\n",
        "          idx.extend(bat)\n",
        "\n",
        "      # Save embeddings with a unique name\n",
        "      embed_file = f\"{parent_path}/Results/Net2Net-NE/embed_link_pred_{graph.split('.')[0]}_{text_filename.split('.')[0]}.txt\"\n",
        "      #embed_file = f\"{parent_path}/Results/Net2Net-NE/embed_node_clf_{graph.split('.')[0]}_{text_filename.split('.')[0]}.txt\"\n",
        "\n",
        "      with open(embed_file, 'wb') as f:\n",
        "        for i in range(len(idx)):\n",
        "          if hidden[i]:\n",
        "              f.write((' '.join(map(str, hidden)) + '\\n').encode())\n",
        "          else:\n",
        "              f.write('\\n'.encode()) # For link prediction\n",
        "              f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c726e8",
      "metadata": {
        "id": "69c726e8"
      },
      "source": [
        "# ***Train(Single Execution)***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read graph\n",
        "graph = MyGraph(f'{parent_path}/{graph_file}')\n",
        "\n",
        "# Read node labels\n",
        "_, labels = read_node_label(f'{parent_path}/{categories_file}')\n",
        "\n",
        "# Read node content (abstracts) and vocabulary of contents\n",
        "node_content, pad_code, word_num = read_word_code(f'{parent_path}/{data_text_file}', f'{parent_path}/{vocab_file}')\n",
        "\n",
        "features = ContentCNN(word_num, word_emb_dim, conv_dim, kernel_num, kernel_sizes, conv_drop, gpu)\n",
        "\n",
        "agg1 = MeanAggregator(lambda nodes: features(fetch(node_content, nodes, MAX_LEN, pad_code)), gpu)\n",
        "enc1 = EgoEncoder(lambda nodes: features(fetch(node_content, nodes, MAX_LEN, pad_code)), conv_dim, enc_dim, graph, agg1)\n",
        "\n",
        "agg2 = MeanAggregator(lambda nodes: enc1(nodes), gpu)\n",
        "enc2 = EgoEncoder(lambda nodes: enc1(nodes), enc1.embed_dim, enc_dim, graph, agg2, base_model=enc1)\n",
        "\n",
        "c2n = Net2Net(graph, features, enc2)\n",
        "c2n.cuda(gpu)\n",
        "\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, c2n.parameters()), lr=l_rate)\n",
        "\n",
        "batch_list = graph.get_batches(batch_size)"
      ],
      "metadata": {
        "id": "j9UrHoqxzz9u",
        "outputId": "23ed49e1-673d-410b-86b8-c2db2a9983f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j9UrHoqxzz9u",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 11534\n",
            "Node contents: \n",
            "1990\n",
            "239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67005dd",
      "metadata": {
        "id": "a67005dd"
      },
      "outputs": [],
      "source": [
        "start_time = datetime.now()\n",
        "for e in range(epoch_num):\n",
        "    avg_loss = []\n",
        "    c2n.train()\n",
        "\n",
        "    for batch in batch_list:\n",
        "        optimizer.zero_grad()\n",
        "        loss = c2n.loss(batch, torch.tensor(batch, dtype=torch.int64, device=gpu))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss.append(loss.item())\n",
        "\n",
        "    print(f'Epoch: {e}')\n",
        "    # Node classification results per epoch\n",
        "    ''' res = c2n.evaluate(batch_list, labels, clf_ratio)\n",
        "    total_time = np.around((time.time() - start) / 60.0)\n",
        "    ls = np.mean(avg_loss)\n",
        "    print('Epoch:', e, 'Loss:', ls, 'micro-F1:', np.around(res, 3), 'Time:', total_time, 'mins.') '''\n",
        "    avg_loss.clear()\n",
        "\n",
        "end_time = datetime.now()\n",
        "print(f'Total Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "res = c2n.evaluate(batch_list, labels, clf_ratio)\n",
        "print('micro-F1:', np.around(res, 3))\n",
        "#c2n.evaluate_and_save_embeddings(batch_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ed9c0b",
      "metadata": {
        "id": "53ed9c0b"
      },
      "source": [
        "## Link Prediction (For both single and multiple executions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021de79d",
      "metadata": {
        "id": "021de79d"
      },
      "outputs": [],
      "source": [
        "embed_files = [\n",
        "    [\n",
        "        f\"{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph{size}_{method}.txt\"\n",
        "        for method in [\"data-v3-500\", \"data-v3-500C\", \"YAKE10\", \"PosR10\", \"PosR5\", \"YAKE5\", \"RAKE10\", \"RAKE5\", \"RAKE10C\", \"RAKE5C\", \"TFIDF5\", \"TFIDF10\", \"TextR10\", \"TextR5\", \"TopicR10\", \"TopicR5\"]\n",
        "    ]\n",
        "    for size in [\"15\", \"45\", \"75\"]\n",
        "]\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(f'{parent_path}/Results/Net2Net-NE/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode() != '\\n':\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split()))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j])) # All the unique nodes in \"edges\"\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/Net2Net-NE/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583f2fb2",
      "metadata": {
        "id": "583f2fb2"
      },
      "source": [
        "## Node Classification (For both single and multiple executions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6741ae27",
      "metadata": {
        "id": "6741ae27"
      },
      "outputs": [],
      "source": [
        "embed_files = [\n",
        "    f\"{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_{method}.txt\"\n",
        "    for method in [\"data-v3-500\", \"data-v3-500C\", \"YAKE10\", \"PosR10\", \"PosR5\", \"YAKE5\", \"RAKE10\", \"RAKE5\", \"RAKE10C\", \"RAKE5C\", \"TFIDF5\", \"TFIDF10\", \"TextR10\", \"TextR5\", \"TopicR10\", \"TopicR5\"]\n",
        "]\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "\n",
        "  for ef in embed_files:\n",
        "    X = []\n",
        "    Y = []\n",
        "    new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "    for jk in range(0, clf_test_len):\n",
        "      if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "        # Y.append([(int)(i) for i in tags])\n",
        "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "        if len(lli) != 0:\n",
        "          if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "            X.append(jk)\n",
        "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "    # This part of the code uses only the X and Y lists created above\n",
        "    mi = {}\n",
        "    ma = {}\n",
        "    li1 = []\n",
        "    li2 = []\n",
        "    with open(f'{parent_path}/Results/DeepEmLAN/{node_clf_results_file}', 'a') as f:\n",
        "\n",
        "      f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "      print(ef.split('/')[-1])\n",
        "\n",
        "      for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "        for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "          clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                          clf=LogisticRegression())\n",
        "\n",
        "          result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "          # Results\n",
        "          li1.append(result['micro'])\n",
        "          li2.append(result['macro'])\n",
        "\n",
        "        mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "        ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "        print(mi)\n",
        "        print(ma)\n",
        "        print()\n",
        "\n",
        "        f.writelines(str(str(mi)+str(ma)))\n",
        "        f.write('\\n')\n",
        "\n",
        "        # Reinitialize the dictionaries and lists\n",
        "        mi = {}\n",
        "        ma = {}\n",
        "        li1 = []\n",
        "        li2 = []\n",
        "\n",
        "    gc.collect()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}