{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18f22bd",
   "metadata": {},
   "source": [
    "# ***Libraries & Tools***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2235cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 2\n",
    "gpu = torch.device('cuda', gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58f741",
   "metadata": {},
   "source": [
    "# ***Global Variables & General Functionality***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"arxiv\"\n",
    "parent_path = f'Datasets/{dataset_name}/graph-v2'\n",
    "graph_file = 'graph.txt'\n",
    "categories_file = 'group-v2.txt'\n",
    "\n",
    "data_text_file  = \"data-v3-500.txt\" \n",
    "data_text_files = [\"data-v3-500.txt\", \"data-v3-500C.txt\", \"YAKE10.txt\", \"YAKE5.txt\", \"RAKE10.txt\", \"RAKE5.txt\", \"RAKE10C.txt\", \"RAKE5C.txt\", \"TFIDF10.txt\", \"TFIDF5.txt\", \"PosR5.txt\",\n",
    "                   \"PosR10.txt\", \"TextR5.txt\", \"TextR10.txt\", \"TopicR5.txt\", \"TopicR10.txt\"]\n",
    "\n",
    "vocab_file = 'vocab.txt'\n",
    "\n",
    "log_file               = 'Net2Net-NE_Execution_Logs.txt'\n",
    "link_pred_results_file = 'Net2Net-NE_Link_Pred_Res.txt'\n",
    "node_clf_results_file  = 'Net2Net-NE_Node_Clf_Res.txt'\n",
    "\n",
    "\n",
    "split_graph_file  = 'sgraph15.txt' \n",
    "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
    "test_graph_file   = 'tgraph85.txt' \n",
    "test_graph_files  = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']\n",
    "\n",
    "\n",
    "word_num = 12619\n",
    "MAX_LEN = 300 # Default value for single execution\n",
    "MAX_LENS = [] # List to hold values for multiple executions\n",
    "\n",
    "word_emb_dim = 500\n",
    "conv_dim = 500\n",
    "kernel_num = 200\n",
    "kernel_sizes = [1, 2, 3, 4, 5]\n",
    "conv_drop = 0.2\n",
    "enc_dim = 500\n",
    "batch_size = 64\n",
    "epoch_num = 50\n",
    "l_rate = 1e-3 \n",
    "clf_ratio = [0.15, 0.45, 0.75]\n",
    "clf_num = 5\n",
    "train_classifier = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average number of words from each data text file\n",
    "for txtf in data_text_files: # 1) ['data-v3-500.txt.txt'] 2) data_text_files:\n",
    "    total_word_count = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
    "            total_lines += 1\n",
    "\n",
    "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
    "    MAX_LENS.append(int(math.ceil(mean_word_count)))\n",
    "    print(f'=== {txtf} ===')\n",
    "    print(\"Mean word count:\", math.ceil(mean_word_count))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b991536",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = MAX_LENS[-1] # For single execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_from_file(file_path):\n",
    "  vectors = {}\n",
    "\n",
    "  with open(f'{file_path}', \"r\", encoding='utf-8') as f:\n",
    "      for idx, line in enumerate(f):\n",
    "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
    "          vectors[idx] = vector  # Assign embedding to node idx\n",
    "\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_list = []\n",
    "for i in range(0, word_emb_dim):\n",
    "    zero_list.append(0)\n",
    "zero_list = np.array(zero_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c634c",
   "metadata": {},
   "source": [
    "# ***Classify***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return np.asarray(all_labels)\n",
    "\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[x] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        # averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        # f1_results = {}\n",
    "        # pre_results = {}\n",
    "        # rec_results = {}\n",
    "        # acc_results = accuracy_score(Y, Y_)\n",
    "        # f1_macro = f1_score(Y, Y_, average=\"macro\")\n",
    "        f1_micro = f1_score(Y, Y_, average=\"micro\")\n",
    "        # for average in averages:\n",
    "        #      f1_results[average] = f1_score(Y, Y_, average=average)\n",
    "        #     pre_results[average] = precision_score(Y, Y_, average=average)\n",
    "        #     rec_results[average] = recall_score(Y, Y_, average=average)\n",
    "        # print 'Results, using embeddings of dimensionality', len(self.embeddings[X[0]])\n",
    "        # print '-------------------'\n",
    "        # print('\\nF1 Score: ')\n",
    "        # print(f1_results)\n",
    "        # print('\\nPrecision Score:')\n",
    "        # print(pre_results)\n",
    "        # print('\\nRecall Score:')\n",
    "        # print(rec_results)\n",
    "        # print('Accuracy Score:', acc_results)\n",
    "\n",
    "        # return f1_results, pre_results, rec_results, acc_results\n",
    "        return f1_micro\n",
    "        # print '-------------------'\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = np.asarray([self.embeddings[x] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = np.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        np.random.seed(seed)\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        np.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    fin = open(filename, 'r')\n",
    "    node_num, size = [int(x) for x in fin.readline().strip().split()]\n",
    "    vectors = {}\n",
    "    while 1:\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "        assert len(vec) == size + 1\n",
    "        vectors[vec[0]] = [float(x) for x in vec[1:]]\n",
    "    fin.close()\n",
    "    assert len(vectors) == node_num\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def read_node_label(filename):\n",
    "    fin = open(filename, 'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "\n",
    "        if len(vec) == 2:\n",
    "            X.append(int(vec[0]))\n",
    "            Y.append([int(v) for v in vec[1:]])\n",
    "    fin.close()\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd35348",
   "metadata": {},
   "source": [
    "# ***Utilities***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd874fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read node features from file\n",
    "def read_node_fea(feature_path):\n",
    "    fea = []\n",
    "    fin = open(feature_path, 'r')\n",
    "    for l in fin.readlines():\n",
    "        vec = l.split()\n",
    "        fea.append(np.array([float(x) for x in vec[1:]]))\n",
    "    fin.close()\n",
    "    return np.array(fea, dtype='float32')\n",
    "\n",
    "\n",
    "def read_word_code(text_path, voca_path):\n",
    "    words = []\n",
    "    fin = open(voca_path, 'r')\n",
    "    for l in fin.readlines():\n",
    "        words.append(l.strip())\n",
    "    fin.close()\n",
    "    word_map = {words[i]: i for i in range(len(words))}\n",
    "    pad_code = word_map['<eos>']\n",
    "\n",
    "    content_code = []\n",
    "    fin = open(text_path, 'r')\n",
    "    for l in fin.readlines():\n",
    "        info = l.strip().split(' ')\n",
    "        doc_code = [word_map[w] for w in info]\n",
    "        # if len(doc_code) > max_len:\n",
    "        #     doc_code = doc_code[0: max_len]\n",
    "        # else:\n",
    "        #     doc_code.extend([pad_code for _ in range(max_len - len(doc_code))])\n",
    "        content_code.append(doc_code)\n",
    "    return content_code, pad_code\n",
    "    # return np.array(content_code, dtype='int')\n",
    "\n",
    "\n",
    "def fetch(content_code, ids, max_len, pad_code):\n",
    "    code = []\n",
    "    for id in ids:\n",
    "        doc_code = content_code[id]\n",
    "        if len(doc_code) > max_len:\n",
    "            doc_code = doc_code[0: max_len]\n",
    "        else:\n",
    "            doc_code.extend([pad_code for _ in range(max_len - len(doc_code))])\n",
    "        code.append(doc_code)\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def node_classification(hidden, idx, label, ratio):\n",
    "    lr = Classifier(vectors=hidden, clf=LogisticRegression())\n",
    "    f1_mi = lr.split_train_evaluate(idx, label, ratio)\n",
    "    return f1_mi\n",
    "\n",
    "\n",
    "def exclusive_combine(*in_list):\n",
    "    res = set()\n",
    "    in_list = list(*in_list)\n",
    "    for n_l in in_list:\n",
    "        for i in n_l:\n",
    "            res.add(i)\n",
    "    return list(res)\n",
    "\n",
    "\n",
    "def identity_map(n_list):\n",
    "    id_dict = {}\n",
    "    for i in range(len(n_list)):\n",
    "        id_dict[n_list[i]] = i\n",
    "    return id_dict\n",
    "\n",
    "\n",
    "def agg_mean(M, id_dict, keys):\n",
    "    idList = []\n",
    "    for id in keys:\n",
    "        idList.append(id_dict[id])\n",
    "\n",
    "    return torch.mean(M[idList, :], 0, True)\n",
    "\n",
    "\n",
    "def agg_max(M, id_dict, keys):\n",
    "    idList = []\n",
    "    for id in keys:\n",
    "        idList.append(id_dict[id])\n",
    "    res, _ = torch.max(M[idList, :], 0, True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a58219",
   "metadata": {},
   "source": [
    "# ***Graph***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGraph(object):\n",
    "    def __init__(self, path, edgelist=True):\n",
    "        self.neighbor_dict = {}\n",
    "        if edgelist:\n",
    "            fin = open(path, 'r')\n",
    "            for l in fin.readlines():\n",
    "                e = l.split()\n",
    "                i, j = int(e[0]), int(e[1])\n",
    "                # Undirected edges\n",
    "                self.update_edge(i, j)\n",
    "                self.update_edge(j, i)\n",
    "            fin.close()\n",
    "\n",
    "        # Convert node's neighbors from dict to list\n",
    "        for key in self.neighbor_dict.keys():\n",
    "            self.neighbor_dict[key] = list(self.neighbor_dict[key])\n",
    "\n",
    "        self.node_list = list(self.neighbor_dict.keys())\n",
    "        self.node_list.sort()\n",
    "        self.node_num = len(self.node_list)\n",
    "\n",
    "    def update_edge(self, i, j):\n",
    "        if i in self.neighbor_dict:\n",
    "            self.neighbor_dict[i].add(j)\n",
    "        else:\n",
    "            self.neighbor_dict[i] = {j}\n",
    "\n",
    "        if j in self.neighbor_dict:\n",
    "            self.neighbor_dict[j].add(i)\n",
    "        else:\n",
    "            self.neighbor_dict[j] = {i}\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        # np.random.seed(1)\n",
    "        np.random.shuffle(self.node_list)\n",
    "        num_batches = self.node_num // batch_size\n",
    "        batch_list = []\n",
    "\n",
    "        # Create \"num_batches\" number of batches\n",
    "        for n in range(num_batches):\n",
    "            batch_list.append(self.node_list[n * batch_size: (n + 1) * batch_size])\n",
    "        \n",
    "        # Create a final batch that contains the remaining nodes\n",
    "        if self.node_num > num_batches * batch_size:\n",
    "            batch_list.append(self.node_list[num_batches * batch_size:])\n",
    "\n",
    "        self.node_list.sort()\n",
    "        return batch_list\n",
    "\n",
    "    def get_neighbors(self, in_list):\n",
    "        neighbors = [self.neighbor_dict[i] for i in in_list]\n",
    "        return exclusive_combine(neighbors)\n",
    "\n",
    "    def diffuse(self, step, nodes):\n",
    "        cur_list = nodes\n",
    "        scale_list = [cur_list]\n",
    "        for s in range(step):\n",
    "            neighbors = self.get_neighbors(cur_list)\n",
    "            cur_list = exclusive_combine([cur_list, neighbors])\n",
    "            scale_list.append(cur_list)\n",
    "        return scale_list  # From now to the past\n",
    "\n",
    "    def statistic(self):\n",
    "        neigh_num = []\n",
    "        for n in self.node_list:\n",
    "            neigh_num.append(len(self.neighbor_dict[n]))\n",
    "\n",
    "        return np.max(neigh_num), np.min(neigh_num), np.mean(neigh_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b3269",
   "metadata": {},
   "source": [
    "# ***Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module):\n",
    "\n",
    "    def __init__(self, features, cur_device, gcn=False):\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "        self.features = features\n",
    "        self.device = cur_device\n",
    "        self.gcn = gcn\n",
    "        \n",
    "    def forward(self, nodes, to_neighs):\n",
    "        samp_neighs = [samp_neigh + [nodes[i]] for i, samp_neigh in enumerate(to_neighs)]\n",
    "\n",
    "        unique_nodes_list = exclusive_combine(samp_neighs)\n",
    "        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n",
    "        # The mask for aggregation\n",
    "        mask = torch.zeros(len(samp_neighs), len(unique_nodes), requires_grad=False, device=self.device)\n",
    "        # The connections\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        # Normalize\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "\n",
    "        embed_matrix = self.features(unique_nodes_list)\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats  # node_num * fea_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd28ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EgoEncoder(nn.Module):\n",
    "    def __init__(self, features, feature_dim, embed_dim, graph, aggregator, base_model=None):\n",
    "        super(EgoEncoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.graph = graph\n",
    "        self.aggregator = aggregator\n",
    "        if base_model is not None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(self.feat_dim, embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        to_neighs = [self.graph.neighbor_dict[node] for node in nodes]\n",
    "        neigh_feats = self.aggregator.forward(nodes, to_neighs)\n",
    "        combined = neigh_feats\n",
    "        combined.mm(self.weight)\n",
    "        combined = torch.tanh(combined)\n",
    "        return combined  # node_num * emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentCNN(nn.Module):\n",
    "    def __init__(self, word_num, word_emb_dim, conv_dim, kernel_num, kernel_sizes, dropout, cur_device):\n",
    "        super(ContentCNN, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(word_num, word_emb_dim)\n",
    "        # self.word_embeddings.weight = nn.Parameter(torch.FloatTensor(word_num, word_emb_dim))\n",
    "        # self.word_embeddings.cuda(cur_device)\n",
    "\n",
    "        # CNN with different kernel sizes\n",
    "        self.conv_list = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, word_emb_dim)) for K in kernel_sizes])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.fc = nn.Linear(len(kernel_sizes) * kernel_num, conv_dim)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(len(kernel_sizes) * kernel_num, conv_dim))\n",
    "        self.device = cur_device\n",
    "\n",
    "        init.xavier_uniform(self.word_embeddings.weight)\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x_conv = conv(x)\n",
    "        x_act = F.relu(x_conv).squeeze(3)  # (N, Co, W)\n",
    "        x_pool = F.max_pool1d(x_act, x_act.size(2)).squeeze(2)\n",
    "        return x_pool\n",
    "\n",
    "    def forward(self, node_batch):\n",
    "        query = torch.LongTensor(node_batch).cuda(self.device)\n",
    "        x = self.word_embeddings(query)  # (N, W, D)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv_list]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        # logit = self.fc(x)  # (N, C)\n",
    "        logit = x.mm(self.weight)\n",
    "        logit = torch.tanh(logit)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcacada",
   "metadata": {},
   "source": [
    "# ***Net2Net-NE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2Net(nn.Module):\n",
    "    def __init__(self, global_graph, features, encoder):\n",
    "        super(Net2Net, self).__init__()\n",
    "        self.graph = global_graph\n",
    "        self.node_num = self.graph.node_num\n",
    "        self.embed_dim = encoder.embed_dim\n",
    "        self.features = features\n",
    "        self.encoder = encoder\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(self.embed_dim, self.node_num))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.encoder(nodes)\n",
    "        scores = embeds.mm(self.weight)\n",
    "        return scores\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())\n",
    "\n",
    "    def evaluate(self, b_list, lab, ratio):\n",
    "        self.eval()\n",
    "        hidden = []\n",
    "        idx = []\n",
    "        for bat in b_list:\n",
    "            h = self.encoder(bat)\n",
    "            hidden.extend(h.detach().cpu().numpy())\n",
    "            idx.extend(bat)\n",
    "\n",
    "        f1 = []\n",
    "        for r in ratio:\n",
    "            f1.append(node_classification(hidden, np.arange(len(lab)), [lab[i] for i in idx], r))\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c726e8",
   "metadata": {},
   "source": [
    "# ***Train(Single Execution)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67005dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Read graph\n",
    "graph = MyGraph(f'{parent_path}/{graph_file}')\n",
    "\n",
    "# Read node labels\n",
    "_, labels = read_node_label(f'{parent_path}/{categories_file}')\n",
    "\n",
    "# Read node content (abstracts) and vocabulary of contents\n",
    "node_content, pad_code = read_word_code(f'{parent_path}/{data_text_file}', f'{parent_path}/{vocab_file}')\n",
    "\n",
    "features = ContentCNN(word_num, word_emb_dim, conv_dim, kernel_num, kernel_sizes, conv_drop, gpu)\n",
    "\n",
    "agg1 = MeanAggregator(lambda nodes: features(fetch(node_content, nodes, MAX_LEN, pad_code)), gpu)\n",
    "enc1 = EgoEncoder(lambda nodes: features(fetch(node_content, nodes, MAX_LEN, pad_code)), conv_dim, enc_dim, graph, agg1)\n",
    "\n",
    "agg2 = MeanAggregator(lambda nodes: enc1(nodes), gpu)\n",
    "enc2 = EgoEncoder(lambda nodes: enc1(nodes), enc1.embed_dim, enc_dim, graph, agg2, base_model=enc1)\n",
    "\n",
    "c2n = Net2Net(graph, features, enc2)\n",
    "c2n.cuda(gpu)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, c2n.parameters()), lr=l_rate)\n",
    "\n",
    "for e in range(epoch_num):\n",
    "    avg_loss = []\n",
    "    c2n.train()\n",
    "    batch_list = graph.get_batches(batch_size)\n",
    "    for batch in batch_list:\n",
    "        optimizer.zero_grad()\n",
    "        loss = c2n.loss(batch, torch.tensor(batch, dtype=torch.int64, device=gpu))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss.append(loss.item())\n",
    "\n",
    "    # Node classification results\n",
    "    f1_micro = c2n.evaluate(batch_list, labels, class_ratio)\n",
    "    minute = np.around((time.time() - start) / 60)\n",
    "    ls = np.mean(avg_loss)\n",
    "    print('Epoch:', e, 'loss:', ls, 'mi-F1:', np.around(f1_micro, 3), 'time:', minute, 'mins.')\n",
    "    avg_loss.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed9c0b",
   "metadata": {},
   "source": [
    "## Link Prediction (For both single and multiple executions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021de79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_files = [[f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_data-v3-500.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_data-v3-500C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_YAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_PosR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_PosR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_YAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_RAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_RAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_RAKE10C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_RAKE5C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_TFIDF5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_TFIDF10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_TextR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_TextR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_TopicR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph15_TopicR5.txt'],\n",
    "\n",
    "               [f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_data-v3-500.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_data-v3-500C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_YAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_PosR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_PosR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_YAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_RAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_RAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_RAKE10C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_RAKE5C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_TFIDF5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_TFIDF10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_TextR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_TextR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_TopicR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph45_TopicR5.txt'],\n",
    "\n",
    "               [f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_data-v3-500.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_data-v3-500C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_YAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_PosR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_PosR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_YAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_RAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_RAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_RAKE10C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_RAKE5C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_TFIDF5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_TFIDF10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_TextR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_TextR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_TopicR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_link_pred_sgraph75_TopicR5.txt']]\n",
    "\n",
    "\n",
    "# Initialize a log file to store the AUC results\n",
    "with open(f'{parent_path}/Results/Net2Net-NE/{link_pred_results_file}', \"a\") as f:\n",
    "    f.write(\"Embed File\\tAUC Value\\n\")\n",
    "\n",
    "for tgfi, tgf in enumerate(test_graph_files):\n",
    "  for ef in embed_files[tgfi]:\n",
    "      node2vec = {}\n",
    "\n",
    "      # Load the embeddings from the current embed file\n",
    "      with open(ef, 'rb') as f:\n",
    "          for i, j in enumerate(f):\n",
    "              if j.decode() != '\\n':\n",
    "                  node2vec[i] = list(map(float, j.strip().decode().split()))\n",
    "\n",
    "      # Load the edges from the test graph file\n",
    "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
    "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
    "\n",
    "      nodes = list(set([i for j in edges for i in j])) # All the unique nodes in \"edges\"\n",
    "\n",
    "      # Calculate AUC\n",
    "      a = 0\n",
    "      b = 0\n",
    "      for i, j in edges:\n",
    "          if i in node2vec.keys() and j in node2vec.keys():\n",
    "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
    "              random_node = random.sample(nodes, 1)[0]\n",
    "              while random_node == j or random_node not in node2vec.keys():\n",
    "                  random_node = random.sample(nodes, 1)[0]\n",
    "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
    "              if dot1 > dot2:\n",
    "                  a += 1\n",
    "              elif dot1 == dot2:\n",
    "                  a += 0.5\n",
    "              b += 1\n",
    "\n",
    "      auc_value = float(a) / b if b > 0 else 0\n",
    "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
    "\n",
    "      # Log the result\n",
    "      with open(f'{parent_path}/Results/Net2Net-NE/{link_pred_results_file}', \"a\") as f:\n",
    "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
    "\n",
    "      gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f2fb2",
   "metadata": {},
   "source": [
    "## Node Classification (For both single and multiple executions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_files = [f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_data-v3-500.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_data-v3-500C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_YAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_PosR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_PosR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_YAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_RAKE10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_RAKE5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_RAKE10C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_RAKE5C.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_TFIDF5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_TFIDF10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_TextR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_TextR5.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_TopicR10.txt',\n",
    "               f'{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_TopicR5.txt']\n",
    "\n",
    "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
    "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
    "\n",
    "if train_classifier:\n",
    "\n",
    "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
    "\n",
    "  for ef in embed_files:\n",
    "    X = []\n",
    "    Y = []\n",
    "    new_vector = get_vectors_from_file(ef)\n",
    "\n",
    "    for jk in range(0, clf_test_len):\n",
    "      if str(jk) in nodes: # If the index \"jk\" is a node\n",
    "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
    "        # Y.append([(int)(i) for i in tags])\n",
    "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
    "        if len(lli) != 0:\n",
    "          if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
    "            X.append(jk)\n",
    "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
    "\n",
    "    # This part of the code uses only the X and Y lists created above\n",
    "    mi = {}\n",
    "    ma = {}\n",
    "    li1 = []\n",
    "    li2 = []\n",
    "    with open(f'{parent_path}/Results/DeepEmLAN/{node_clf_results_file}', 'a') as f:\n",
    "\n",
    "      f.write(f\"{ef.split('/')[-1]} \\n\")\n",
    "      print(ef.split('/')[-1])\n",
    "\n",
    "      for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
    "        for j in range(0, clf_num): # clf_num = 5\n",
    "\n",
    "          clf = Classifier(vectors=new_vector, # All node embeddings\n",
    "                          clf=LogisticRegression())\n",
    "\n",
    "          result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
    "\n",
    "          # Results\n",
    "          li1.append(result['micro'])\n",
    "          li2.append(result['macro'])\n",
    "\n",
    "        mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
    "        ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
    "\n",
    "        print(mi)\n",
    "        print(ma)\n",
    "        print()\n",
    "\n",
    "        f.writelines(str(str(mi)+str(ma)))\n",
    "        f.write('\\n')\n",
    "\n",
    "        # Reinitialize the dictionaries and lists\n",
    "        mi = {}\n",
    "        ma = {}\n",
    "        li1 = []\n",
    "        li2 = []\n",
    "\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
